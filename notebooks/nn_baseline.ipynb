{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4d4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda49740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "030bd08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rimgro/biocadprotein.git\n",
      "  Cloning https://github.com/rimgro/biocadprotein.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-6f677xpx\n",
      "  Resolved https://github.com/rimgro/biocadprotein.git to commit c58a88b9bf0470e92f92d3af4ef70f2f32124fb4\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting pdbfixer@ git+https://github.com/openmm/pdbfixer.git@5a6c129fb5b508610542634eafa218fb0e0671fe\n",
      "  Cloning https://github.com/openmm/pdbfixer.git (to revision 5a6c129fb5b508610542634eafa218fb0e0671fe) to c:\\users\\user\\appdata\\local\\temp\\pip-install-ylot9n93\\pdbfixer_cba43ab6b27f4199be4147d153dbd57f\n",
      "  Resolved https://github.com/openmm/pdbfixer.git to commit 5a6c129fb5b508610542634eafa218fb0e0671fe\n",
      "Collecting esm@ git+https://github.com/evolutionaryscale/esm.git\n",
      "  Cloning https://github.com/evolutionaryscale/esm.git to c:\\users\\user\\appdata\\local\\temp\\pip-install-ylot9n93\\esm_78e74f63dfbe4cdb96f7c36f4b0d10e7\n",
      "  Resolved https://github.com/evolutionaryscale/esm.git to commit be185fb05e1aa3ccfda4e7bd10e1d341f81489a2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: MDAnalysis>=2.9.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from fpgen==0.0.18) (2.9.0)\n",
      "Requirement already satisfied: OpenMM==8.3.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from fpgen==0.0.18) (8.3.0)\n",
      "Requirement already satisfied: py3Dmol>=2.5.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from fpgen==0.0.18) (2.5.1)\n",
      "Requirement already satisfied: httpx>=0.28.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from fpgen==0.0.18) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from fpgen==0.0.18) (2.2.6)\n",
      "Requirement already satisfied: torch>=1.10 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from fpgen==0.0.18) (2.7.1+cu118)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from httpx>=0.28.1->fpgen==0.0.18) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from httpx>=0.28.1->fpgen==0.0.18) (1.0.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from httpx>=0.28.1->fpgen==0.0.18) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from httpx>=0.28.1->fpgen==0.0.18) (2025.6.15)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.1->fpgen==0.0.18) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (3.6.0)\n",
      "Requirement already satisfied: joblib>=0.12 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (3.18.0)\n",
      "Requirement already satisfied: mda-xdrlib in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (1.15.3)\n",
      "Requirement already satisfied: mmtf-python>=1.0.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (1.1.3)\n",
      "Requirement already satisfied: GridDataFormats>=0.4.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (1.0.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (25.0)\n",
      "Requirement already satisfied: matplotlib>=1.5.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (3.10.3)\n",
      "Requirement already satisfied: tqdm>=4.43.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from MDAnalysis>=2.9.0->fpgen==0.0.18) (4.67.1)\n",
      "Requirement already satisfied: mrcfile in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from GridDataFormats>=0.4.0->MDAnalysis>=2.9.0->fpgen==0.0.18) (1.5.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from matplotlib>=1.5.1->MDAnalysis>=2.9.0->fpgen==0.0.18) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from matplotlib>=1.5.1->MDAnalysis>=2.9.0->fpgen==0.0.18) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from matplotlib>=1.5.1->MDAnalysis>=2.9.0->fpgen==0.0.18) (3.2.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from matplotlib>=1.5.1->MDAnalysis>=2.9.0->fpgen==0.0.18) (4.58.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from matplotlib>=1.5.1->MDAnalysis>=2.9.0->fpgen==0.0.18) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from matplotlib>=1.5.1->MDAnalysis>=2.9.0->fpgen==0.0.18) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from matplotlib>=1.5.1->MDAnalysis>=2.9.0->fpgen==0.0.18) (0.12.1)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from mmtf-python>=1.0.0->MDAnalysis>=2.9.0->fpgen==0.0.18) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.5.1->MDAnalysis>=2.9.0->fpgen==0.0.18) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from torch>=1.10->fpgen==0.0.18) (1.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from torch>=1.10->fpgen==0.0.18) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from torch>=1.10->fpgen==0.0.18) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from torch>=1.10->fpgen==0.0.18) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from torch>=1.10->fpgen==0.0.18) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.10->fpgen==0.0.18) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from tqdm>=4.43.0->MDAnalysis>=2.9.0->fpgen==0.0.18) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from anyio->httpx>=0.28.1->fpgen==0.0.18) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from anyio->httpx>=0.28.1->fpgen==0.0.18) (1.3.0)\n",
      "Requirement already satisfied: torchtext in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.18.0)\n",
      "Requirement already satisfied: transformers<4.48.2 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (4.48.1)\n",
      "Requirement already satisfied: tenacity in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (9.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.22.1)\n",
      "Requirement already satisfied: biotite>=1.0.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (1.7.0)\n",
      "Requirement already satisfied: brotli in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (1.1.0)\n",
      "Requirement already satisfied: ipython in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (8.37.0)\n",
      "Requirement already satisfied: msgpack-numpy in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.4.8)\n",
      "Requirement already satisfied: einops in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.8.1)\n",
      "Requirement already satisfied: cloudpathlib in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.21.1)\n",
      "Requirement already satisfied: biopython in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (1.85)\n",
      "Requirement already satisfied: zstd in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (1.5.7.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (2.3.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (25.3.0)\n",
      "Requirement already satisfied: requests>=2.12 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from biotite>=1.0.0->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (2.32.4)\n",
      "Requirement already satisfied: biotraj<2.0,>=1.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from biotite>=1.0.0->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (1.2.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from requests>=2.12->biotite>=1.0.0->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from requests>=2.12->biotite>=1.0.0->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (2.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from transformers<4.48.2->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from transformers<4.48.2->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from transformers<4.48.2->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from transformers<4.48.2->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.33.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from transformers<4.48.2->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (2024.11.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (5.2.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.6.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (2.19.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.19.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (5.14.3)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (3.0.51)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from jedi>=0.16->ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from jinja2->torch>=1.10->fpgen==0.0.18) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from pandas->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from pandas->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (2025.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from stack_data->ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from stack_data->ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (2.2.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\user\\desktop\\biocadprotein\\venv\\lib\\site-packages (from stack_data->ipython->esm@ git+https://github.com/evolutionaryscale/esm.git->fpgen==0.0.18) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/rimgro/biocadprotein.git 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-req-build-6f677xpx'\n",
      "  Running command git clone -q https://github.com/openmm/pdbfixer.git 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-ylot9n93\\pdbfixer_cba43ab6b27f4199be4147d153dbd57f'\n",
      "  Running command git rev-parse -q --verify 'sha^5a6c129fb5b508610542634eafa218fb0e0671fe'\n",
      "  Running command git fetch -q https://github.com/openmm/pdbfixer.git 5a6c129fb5b508610542634eafa218fb0e0671fe\n",
      "  Running command git clone -q https://github.com/evolutionaryscale/esm.git 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-ylot9n93\\esm_78e74f63dfbe4cdb96f7c36f4b0d10e7'\n",
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\User\\Desktop\\biocadprotein\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "! pip install --upgrade git+https://github.com/rimgro/biocadprotein.git\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd10a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Define the amino acid alphabet\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "AA_TO_IDX = {aa: idx for idx, aa in enumerate(AMINO_ACIDS)}\n",
    "IDX_TO_AA = {idx: aa for idx, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for protein sequences and their properties.\n",
    "    \n",
    "    This class handles the conversion from amino acid sequences to numerical\n",
    "    representations and manages the relationship between sequences and their\n",
    "    target properties.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, targets, max_length=500):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            sequences: List of protein sequences as strings\n",
    "            targets: List of target values (e.g., excitation wavelengths)\n",
    "            max_length: Maximum sequence length for padding/truncation\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Normalize targets using z-score normalization\n",
    "        self.target_scaler = StandardScaler()\n",
    "        self.normalized_targets = self.target_scaler.fit_transform(\n",
    "            np.array(targets).reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sequence-target pair.\n",
    "        \n",
    "        Returns:\n",
    "            sequence_tensor: One-hot encoded sequence [max_length, 20]\n",
    "            target_tensor: Normalized target value\n",
    "        \"\"\"\n",
    "        sequence = self.sequences[idx]\n",
    "        target = self.normalized_targets[idx]\n",
    "        \n",
    "        # Convert sequence to one-hot encoding\n",
    "        sequence_tensor = self.sequence_to_tensor(sequence)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "        \n",
    "        return sequence_tensor, target_tensor\n",
    "    \n",
    "    def sequence_to_tensor(self, sequence):\n",
    "        \"\"\"\n",
    "        Convert a protein sequence string to a one-hot encoded tensor.\n",
    "        \n",
    "        This function handles padding and truncation to ensure all sequences\n",
    "        have the same length.\n",
    "        \"\"\"\n",
    "        # Truncate if too long\n",
    "        if len(sequence) > self.max_length:\n",
    "            sequence = sequence[:self.max_length]\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        tensor = torch.zeros(self.max_length, len(AMINO_ACIDS))\n",
    "        \n",
    "        for i, aa in enumerate(sequence):\n",
    "            if aa in AA_TO_IDX:\n",
    "                tensor[i, AA_TO_IDX[aa]] = 1.0\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def denormalize_target(self, normalized_value):\n",
    "        \"\"\"\n",
    "        Convert normalized target back to original scale.\n",
    "        \n",
    "        This is useful for interpreting model predictions.\n",
    "        \"\"\"\n",
    "        return self.target_scaler.inverse_transform([[normalized_value]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ddebc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for protein sequence analysis.\n",
    "    \n",
    "    This network uses multiple convolutional layers with different filter sizes\n",
    "    to capture local patterns in protein sequences, followed by fully connected\n",
    "    layers to make the final prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_amino_acids=20, num_filters=128, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the CNN architecture.\n",
    "        \n",
    "        Args:\n",
    "            num_amino_acids: Size of amino acid vocabulary (20 for standard amino acids)\n",
    "            num_filters: Number of convolutional filters for each filter size\n",
    "            dropout_rate: Dropout probability for regularization\n",
    "        \"\"\"\n",
    "        super(ProteinCNN, self).__init__()\n",
    "        \n",
    "        # Multiple convolutional layers with different filter sizes\n",
    "        # This allows us to capture patterns of different lengths\n",
    "        self.conv1 = nn.Conv1d(num_amino_acids, num_filters, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(num_amino_acids, num_filters, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(num_amino_acids, num_filters, kernel_size=7, padding=3)\n",
    "        \n",
    "        # Batch normalization helps with training stability\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(num_filters)\n",
    "        \n",
    "        # Additional convolutional layers for more complex pattern detection\n",
    "        self.conv4 = nn.Conv1d(num_filters * 3, num_filters * 2, kernel_size=3, padding=1)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(num_filters * 2)\n",
    "        \n",
    "        self.conv5 = nn.Conv1d(num_filters * 2, num_filters, kernel_size=3, padding=1)\n",
    "        self.batch_norm5 = nn.BatchNorm1d(num_filters)\n",
    "        \n",
    "        # Global pooling to get a fixed-size representation\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully connected layers for final prediction\n",
    "        self.fc1 = nn.Linear(num_filters, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)  # Single output for regression\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, sequence_length, num_amino_acids]\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Tensor of shape [batch_size, 1] with predicted values\n",
    "        \"\"\"\n",
    "        # Transpose for conv1d: [batch_size, num_amino_acids, sequence_length]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Apply multiple convolutional layers with different filter sizes\n",
    "        conv1_out = F.relu(self.batch_norm1(self.conv1(x)))\n",
    "        conv2_out = F.relu(self.batch_norm2(self.conv2(x)))\n",
    "        conv3_out = F.relu(self.batch_norm3(self.conv3(x)))\n",
    "        \n",
    "        # Concatenate outputs from different filter sizes\n",
    "        # This gives us a rich representation that captures patterns of various lengths\n",
    "        x = torch.cat([conv1_out, conv2_out, conv3_out], dim=1)\n",
    "        \n",
    "        # Apply additional convolutional layers\n",
    "        x = F.relu(self.batch_norm4(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.batch_norm5(self.conv5(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Global pooling to get a fixed-size representation regardless of sequence length\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        # Fully connected layers for final prediction\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Final prediction (no activation for regression)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c2f0fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# This ensures that your results are consistent across runs\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a45656fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the CNN model with careful monitoring and early stopping.\n",
    "    \n",
    "    This function implements best practices for training neural networks on small datasets,\n",
    "    including learning rate scheduling, early stopping, and comprehensive monitoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model to the appropriate device (GPU if available)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Choose optimizer and loss function\n",
    "    # Adam is generally a good choice for CNNs because it adapts the learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # Mean Squared Error for regression tasks\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Learning rate scheduler - reduces learning rate when validation loss plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=10, factor=0.5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "    \n",
    "    # Track training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Validation batches per epoch: {len(val_loader)}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(sequences)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions.squeeze(), targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}')\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                predictions = model(sequences)\n",
    "                loss = criterion(predictions.squeeze(), targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        \n",
    "        # Record history\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.6f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.6f}')\n",
    "        print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.8f}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "            print(f'New best model saved with validation loss: {best_val_loss:.6f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "    \n",
    "    return model, train_losses, val_losses, learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa1b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(sequences, targets, batch_size=32, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Create training and validation data loaders.\n",
    "    \n",
    "    This function handles the train/validation split and creates PyTorch DataLoaders\n",
    "    that will feed batches of data to our model during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = ProteinDataset(sequences, targets, max_length=500)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_split * total_size)\n",
    "    val_size = total_size - train_size\n",
    "    \n",
    "    print(f\"Total dataset size: {total_size}\")\n",
    "    print(f\"Training set size: {train_size}\")\n",
    "    print(f\"Validation set size: {val_size}\")\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,  # Shuffle training data\n",
    "        num_workers=2,  # Use multiple workers for faster data loading\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,  # Don't shuffle validation data\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6790103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model and calculate performance metrics.\n",
    "    \n",
    "    This function provides comprehensive evaluation including predictions\n",
    "    on both normalized and original scales.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in data_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            predictions = model(sequences)\n",
    "            \n",
    "            all_predictions.extend(predictions.squeeze().cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Calculate metrics on normalized scale\n",
    "    mse_normalized = mean_squared_error(all_targets, all_predictions)\n",
    "    r2_normalized = r2_score(all_targets, all_predictions)\n",
    "    \n",
    "    # Denormalize for interpretable metrics\n",
    "    targets_original = [dataset.denormalize_target(t) for t in all_targets]\n",
    "    predictions_original = [dataset.denormalize_target(p) for p in all_predictions]\n",
    "    \n",
    "    mse_original = mean_squared_error(targets_original, predictions_original)\n",
    "    r2_original = r2_score(targets_original, predictions_original)\n",
    "    \n",
    "    print(\"Model Evaluation Results:\")\n",
    "    print(f\"R² Score (original scale): {r2_original:.4f}\")\n",
    "    print(f\"MSE (original scale): {mse_original:.4f}\")\n",
    "    print(f\"RMSE (original scale): {np.sqrt(mse_original):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions_original': predictions_original,\n",
    "        'targets_original': targets_original,\n",
    "        'r2_score': r2_original,\n",
    "        'mse': mse_original,\n",
    "        'rmse': np.sqrt(mse_original)\n",
    "    }\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, learning_rates):\n",
    "    \"\"\"\n",
    "    Visualize the training process to understand model behavior.\n",
    "    \n",
    "    These plots help you understand whether your model is learning properly,\n",
    "    overfitting, or if you need to adjust hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    axes[0, 0].plot(train_losses, label='Training Loss', color='blue')\n",
    "    axes[0, 0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Plot learning rate schedule\n",
    "    axes[0, 1].plot(learning_rates, color='green')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Learning Rate')\n",
    "    axes[0, 1].set_title('Learning Rate Schedule')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Plot loss difference (overfitting indicator)\n",
    "    loss_diff = np.array(val_losses) - np.array(train_losses)\n",
    "    axes[1, 0].plot(loss_diff, color='purple')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Validation Loss - Training Loss')\n",
    "    axes[1, 0].set_title('Overfitting Indicator')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Plot validation loss with trend\n",
    "    axes[1, 1].plot(val_losses, color='red', alpha=0.7)\n",
    "    # Add trend line\n",
    "    z = np.polyfit(range(len(val_losses)), val_losses, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1, 1].plot(range(len(val_losses)), p(range(len(val_losses))), \n",
    "                   color='black', linestyle='--', label='Trend')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Validation Loss')\n",
    "    axes[1, 1].set_title('Validation Loss with Trend')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "858a0cca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fpgen.prop_prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load and prepare data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfpgen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprop_prediction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FPbase\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m FPbase(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fpgen.prop_prediction'"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "from fpgen.prop_prediction.dataset import FPbase\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = FPbase('dataset.csv')\n",
    "\n",
    "# Get train and test data (unscaled for sequences, but we'll scale the targets)\n",
    "x_train, y_train = dataset.get_train('ex_max', is_scaled=False)\n",
    "x_test, y_test = dataset.get_test('ex_max', is_scaled=False)\n",
    "\n",
    "# Scale targets\n",
    "y_train_scaled = dataset.scale_targets(y_train, 'ex_max')\n",
    "y_test_scaled = dataset.scale_targets(y_test, 'ex_max')\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader, val_loader, dataset_obj = setup_data_loaders(\n",
    "    sequences=x_train,\n",
    "    targets=y_train_scaled.squeeze(),  # Remove extra dimension from scaling\n",
    "    batch_size=batch_size,\n",
    "    train_split=0.8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ProteinCNN(\n",
    "    num_amino_acids=20,  # Standard amino acids\n",
    "    num_filters=128,\n",
    "    dropout_rate=0.5\n",
    ").to(device)\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "trained_model, train_losses, val_losses, learning_rates = train_cnn_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data loader\n",
    "test_dataset = ProteinDataset(x_test, y_test_scaled.squeeze(), max_length=500)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "evaluation_results = evaluate_model(trained_model, test_loader, test_dataset)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"R² Score: {evaluation_results['r2_score']:.4f}\")\n",
    "print(f\"RMSE: {evaluation_results['rmse']:.4f}\")\n",
    "print(f\"MSE: {evaluation_results['mse']:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(train_losses, val_losses, learning_rates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
