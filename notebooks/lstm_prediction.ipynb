{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2e0b325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(52)\n",
    "np.random.seed(52)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6bb298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the amino acid alphabet\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "AA_TO_IDX = {aa: idx for idx, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for protein sequences and their properties.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, targets, target_name, max_length=500):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            sequences: List of protein sequences as strings\n",
    "            targets: List of target values\n",
    "            target_name: Name of the target property being predicted\n",
    "            max_length: Maximum sequence length for padding/truncation\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        self.target_name = target_name\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Remove any rows with missing values\n",
    "        valid_indices = []\n",
    "        for i, (seq, target) in enumerate(zip(sequences, targets)):\n",
    "            if pd.notna(seq) and pd.notna(target) and len(seq) > 0:\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        self.sequences = [sequences[i] for i in valid_indices]\n",
    "        self.targets = [targets[i] for i in valid_indices]\n",
    "        \n",
    "        # Normalize targets using z-score normalization\n",
    "        self.target_scaler = StandardScaler()\n",
    "        self.normalized_targets = self.target_scaler.fit_transform(\n",
    "            np.array(self.targets).reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.sequences)} valid samples\")\n",
    "        print(f\"Target property: {target_name}\")\n",
    "        print(f\"Target range: {min(self.targets):.3f} to {max(self.targets):.3f}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single sequence-target pair.\"\"\"\n",
    "        sequence = self.sequences[idx]\n",
    "        target = self.normalized_targets[idx]\n",
    "        \n",
    "        # Convert sequence to integer indices\n",
    "        sequence_tensor = self.sequence_to_indices(sequence)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "        \n",
    "        return sequence_tensor, target_tensor\n",
    "    \n",
    "    def sequence_to_indices(self, sequence):\n",
    "        \"\"\"Convert a protein sequence string to integer indices for embedding.\"\"\"\n",
    "        # Clean sequence - remove any non-amino acid characters\n",
    "        cleaned_sequence = ''.join([aa for aa in sequence.upper() if aa in AMINO_ACIDS])\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(cleaned_sequence) > self.max_length:\n",
    "            cleaned_sequence = cleaned_sequence[:self.max_length]\n",
    "        \n",
    "        # Convert to indices (0 is reserved for padding)\n",
    "        indices = [AA_TO_IDX[aa] + 1 for aa in cleaned_sequence]  # +1 to reserve 0 for padding\n",
    "        \n",
    "        # Pad with zeros to max_length\n",
    "        while len(indices) < self.max_length:\n",
    "            indices.append(0)  # 0 is padding token\n",
    "        \n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "    \n",
    "    def denormalize_target(self, normalized_value):\n",
    "        \"\"\"Convert normalized target back to original scale.\"\"\"\n",
    "        return self.target_scaler.inverse_transform([[normalized_value]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ba41b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinLSTM(nn.Module):\n",
    "    \"\"\"LSTM-based neural network for protein sequence analysis with embedding layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=21, embed_dim=128, hidden_size=256, num_layers=3, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM architecture with embedding.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (21 for 20 amino acids + padding token)\n",
    "            embed_dim: Dimension of embedding vectors\n",
    "            hidden_size: Number of hidden units in LSTM layers\n",
    "            num_layers: Number of LSTM layers\n",
    "            dropout_rate: Dropout probability for regularization\n",
    "        \"\"\"\n",
    "        super(ProteinLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Embedding layer to convert amino acid indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Add positional encoding to help the model understand sequence positions\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 500, embed_dim) * 0.1)\n",
    "        \n",
    "        # Input projection layer\n",
    "        self.input_projection = nn.Linear(embed_dim, hidden_size // 2)\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size // 2,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for sequence-level representation\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2,  # *2 because bidirectional\n",
    "            num_heads=8,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        \n",
    "        # Initialize embeddings with Xavier uniform\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        # Set padding embedding to zero\n",
    "        with torch.no_grad():\n",
    "            self.embedding.weight[0].fill_(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, sequence_length] with amino acid indices\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Tensor of shape [batch_size, 1] with predicted values\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.size()\n",
    "        \n",
    "        # Create mask for padding tokens\n",
    "        mask = (x != 0).float()  # 1 for real tokens, 0 for padding\n",
    "        \n",
    "        # Convert indices to embeddings\n",
    "        x = self.embedding(x)  # [batch_size, seq_length, embed_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        if seq_length <= self.pos_encoding.size(1):\n",
    "            x = x + self.pos_encoding[:, :seq_length, :]\n",
    "        \n",
    "        # Apply mask to embeddings\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "        \n",
    "        # Project input to hidden dimension\n",
    "        x = F.relu(self.input_projection(x))\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Apply mask to LSTM output\n",
    "        lstm_out = lstm_out * mask.unsqueeze(-1)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        \n",
    "        # Self-attention to get sequence-level representation\n",
    "        # Create attention mask for padding\n",
    "        attn_mask = mask.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        attn_mask = attn_mask * attn_mask.transpose(1, 2)\n",
    "        attn_mask = attn_mask.bool()\n",
    "        \n",
    "        attn_out, attn_weights = self.attention(\n",
    "            lstm_out, lstm_out, lstm_out, \n",
    "            key_padding_mask=~mask.bool()\n",
    "        )\n",
    "        \n",
    "        # Apply mask to attention output\n",
    "        attn_out = attn_out * mask.unsqueeze(-1)\n",
    "        \n",
    "        # Global pooling with masking\n",
    "        # Calculate sequence lengths for proper averaging\n",
    "        seq_lengths = mask.sum(dim=1, keepdim=True)  # [batch_size, 1]\n",
    "        \n",
    "        # Mean pooling (sum and divide by actual sequence length)\n",
    "        mean_pool = (attn_out * mask.unsqueeze(-1)).sum(dim=1) / seq_lengths.unsqueeze(-1)\n",
    "        \n",
    "        # Max pooling\n",
    "        attn_out_masked = attn_out.masked_fill(~mask.unsqueeze(-1).bool(), float('-inf'))\n",
    "        max_pool = torch.max(attn_out_masked, dim=1)[0]\n",
    "        \n",
    "        # Combine pooled representations\n",
    "        x = mean_pool + max_pool\n",
    "        \n",
    "        # Output layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be11b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n",
    "    \"\"\"Train the LSTM model.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Training on {device}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (sequences, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences).squeeze()\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences, targets = sequences.to(device), targets.to(device)\n",
    "                predictions = model(sequences).squeeze()\n",
    "                loss = criterion(predictions, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2748daf1-721f-424b-9ba9-18d853ea3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file, target_property):\n",
    "    \"\"\"Load data from CSV file.\"\"\"\n",
    "    print(f\"Loading data from {csv_file}...\")\n",
    "    \n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check if target property exists\n",
    "    if target_property not in df.columns:\n",
    "        raise ValueError(f\"Target property '{target_property}' not found in CSV. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Extract sequences and targets\n",
    "    sequences = df['sequence'].tolist()\n",
    "    targets = df[target_property].tolist()\n",
    "    \n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfab222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, dataset, target_property):\n",
    "    \"\"\"Evaluate the trained model.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in test_loader:\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            batch_predictions = model(sequences).squeeze()\n",
    "            \n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_values.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Convert back to original scale\n",
    "    predictions = np.array(predictions)\n",
    "    true_values = np.array(true_values)\n",
    "    \n",
    "    # Denormalize\n",
    "    predictions_denorm = [dataset.denormalize_target(pred) for pred in predictions]\n",
    "    true_values_denorm = [dataset.denormalize_target(true) for true in true_values]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(true_values_denorm, predictions_denorm)\n",
    "    mae = mean_absolute_error(true_values_denorm, predictions_denorm)\n",
    "    r2 = r2_score(true_values_denorm, predictions_denorm)\n",
    "    \n",
    "    print(f\"\\nModel Evaluation Results for {target_property}:\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return predictions_denorm, true_values_denorm, mse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18daaf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(train_losses, val_losses, predictions, true_values, target_property):\n",
    "    \"\"\"Plot training results and predictions.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training curves\n",
    "    ax1.plot(train_losses, label='Training Loss')\n",
    "    ax1.plot(val_losses, label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot predictions vs true values\n",
    "    ax2.scatter(true_values, predictions, alpha=0.7)\n",
    "    ax2.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], 'r--', lw=2)\n",
    "    ax2.set_xlabel(f'True {target_property}')\n",
    "    ax2.set_ylabel(f'Predicted {target_property}')\n",
    "    ax2.set_title(f'Predictions vs True Values\\n{target_property}')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d288cb-6d42-4041-97a9-baea4958c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpgen.prop_prediction.metrics import get_regression_metrics, get_classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99bd95d0-4c7d-4fc8-ab21-1305ea3a87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(data):\n",
    "    processed = []\n",
    "    for line in data:\n",
    "        clean_line = line.replace('\\n', ' ').strip('[]')\n",
    "        numbers = np.fromstring(clean_line, sep=' ')\n",
    "        processed.append(numbers.tolist())\n",
    "    return np.array(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00e2757f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset_embedd.csv...\n",
      "Loaded 980 rows\n",
      "Columns: ['sequence', 'brightness', 'em_max', 'ex_max', 'ext_coeff', 'lifetime', 'maturation', 'pka', 'stokes_shift', 'qy', 'agg', 'switch_type']\n",
      "Data split: 627 train, 157 validation, 196 test\n",
      "Dataset initialized with 519 valid samples\n",
      "Target property: em_max\n",
      "Target range: 382.000 to 1000.000\n",
      "Dataset initialized with 124 valid samples\n",
      "Target property: em_max\n",
      "Target range: 424.000 to 720.000\n",
      "Dataset initialized with 167 valid samples\n",
      "Target property: em_max\n",
      "Target range: 414.000 to 719.000\n",
      "Training on cuda\n",
      "Number of parameters: 5,473,537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:02<00:00,  6.71it/s]\n",
      "Epoch 2/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.87it/s]\n",
      "Epoch 3/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 4/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 5/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 6/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 7/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 8/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 9/200: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 10/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "Train Loss: 0.9795, Val Loss: 0.9980\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 12/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 13/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 14/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 15/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n",
      "Epoch 16/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 17/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.04it/s]\n",
      "Epoch 18/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 19/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 20/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200\n",
      "Train Loss: 0.9829, Val Loss: 0.9982\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.93it/s]\n",
      "Epoch 22/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 23/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 24/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 25/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 26/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 27/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 28/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 29/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 30/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "Train Loss: 0.9773, Val Loss: 0.9981\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 32/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 33/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 34/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 35/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 36/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 37/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 38/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 39/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 40/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200\n",
      "Train Loss: 1.0257, Val Loss: 0.9981\n",
      "Learning Rate: 0.000250\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 42/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.92it/s]\n",
      "Epoch 43/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 44/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 45/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 46/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.94it/s]\n",
      "Epoch 47/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 48/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 49/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 50/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "Train Loss: 0.9928, Val Loss: 0.9981\n",
      "Learning Rate: 0.000125\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 52/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 53/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 54/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 55/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 56/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 57/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n",
      "Epoch 58/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 59/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 60/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "Train Loss: 0.9753, Val Loss: 0.9981\n",
      "Learning Rate: 0.000063\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 62/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 63/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 64/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 65/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 66/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 67/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 68/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 69/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 70/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200\n",
      "Train Loss: 0.9977, Val Loss: 0.9981\n",
      "Learning Rate: 0.000031\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 72/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 73/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 74/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 75/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 76/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 77/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 78/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.92it/s]\n",
      "Epoch 79/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 80/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "Train Loss: 0.9772, Val Loss: 0.9981\n",
      "Learning Rate: 0.000016\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 82/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.03it/s]\n",
      "Epoch 83/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 84/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 85/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n",
      "Epoch 86/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 87/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 88/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n",
      "Epoch 89/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 90/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200\n",
      "Train Loss: 0.9809, Val Loss: 0.9981\n",
      "Learning Rate: 0.000008\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 92/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 93/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 94/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 95/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 96/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 97/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 98/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 99/200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 100/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "Train Loss: 1.0452, Val Loss: 0.9981\n",
      "Learning Rate: 0.000004\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 102/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 103/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 104/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 105/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 106/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 107/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 108/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 109/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 110/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200\n",
      "Train Loss: 0.9931, Val Loss: 0.9981\n",
      "Learning Rate: 0.000002\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 112/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 113/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 114/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 115/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 116/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 117/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 118/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 119/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 120/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "Train Loss: 1.0174, Val Loss: 0.9981\n",
      "Learning Rate: 0.000001\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 122/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 123/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 124/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 125/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 126/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 127/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 128/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 129/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 130/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200\n",
      "Train Loss: 1.0211, Val Loss: 0.9981\n",
      "Learning Rate: 0.000001\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 132/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 133/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 134/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 135/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 136/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 137/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 138/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 139/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 140/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200\n",
      "Train Loss: 0.9851, Val Loss: 0.9981\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 142/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 143/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 144/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n",
      "Epoch 145/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 146/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 147/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 148/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n",
      "Epoch 149/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 150/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200\n",
      "Train Loss: 0.9625, Val Loss: 0.9981\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 152/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 153/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 154/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 155/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 156/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 157/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 158/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 159/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 160/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200\n",
      "Train Loss: 0.9738, Val Loss: 0.9981\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 162/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 163/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 164/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 165/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 166/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n",
      "Epoch 167/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 168/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 169/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.04it/s]\n",
      "Epoch 170/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200\n",
      "Train Loss: 1.0103, Val Loss: 0.9981\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 172/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 173/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 174/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 175/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n",
      "Epoch 176/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 177/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 178/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 179/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 180/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200\n",
      "Train Loss: 1.0093, Val Loss: 0.9981\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 182/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 183/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 184/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 185/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.94it/s]\n",
      "Epoch 186/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.02it/s]\n",
      "Epoch 187/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 188/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 189/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 190/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/200\n",
      "Train Loss: 0.9935, Val Loss: 0.9981\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 192/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 11.00it/s]\n",
      "Epoch 193/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.97it/s]\n",
      "Epoch 194/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 195/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 196/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.99it/s]\n",
      "Epoch 197/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n",
      "Epoch 198/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.96it/s]\n",
      "Epoch 199/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.98it/s]\n",
      "Epoch 200/200: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200\n",
      "Train Loss: 0.9848, Val Loss: 0.9981\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (167,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     49\u001b[0m trained_model, train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     50\u001b[0m     model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE\n\u001b[1;32m     51\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m predictions1, true_values, mse, mae, r2 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET_PROPERTY\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m     57\u001b[0m plot_results(train_losses, val_losses, predictions1, true_values, TARGET_PROPERTY)\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader, dataset, target_property)\u001b[0m\n\u001b[1;32m     15\u001b[0m         true_values\u001b[38;5;241m.\u001b[39mextend(targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert back to original scale\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predictions)\n\u001b[1;32m     19\u001b[0m true_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(true_values)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Denormalize\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (167,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\"\"\"Main training pipeline.\"\"\"\n",
    "# Configuration\n",
    "CSV_FILE = 'dataset_embedd.csv'  # Update this path if needed\n",
    "TARGET_PROPERTY = 'em_max'  # Change this to predict different properties\n",
    "\n",
    "# Available properties from your dataset:\n",
    "# brightness, ex_max, em_max, ext_coeff, lifetime, maturation, pka, stokes_shift, qy, agg, switch_type\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_LENGTH = 238\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Load data\n",
    "sequences, targets = load_data(CSV_FILE, TARGET_PROPERTY)\n",
    "\n",
    "# Split data\n",
    "train_sequences, test_sequences, train_targets, test_targets = train_test_split(\n",
    "        sequences, targets, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "train_sequences, val_sequences, train_targets, val_targets = train_test_split(\n",
    "    train_sequences, train_targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Data split: {len(train_sequences)} train, {len(val_sequences)} validation, {len(test_sequences)} test\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ProteinDataset(train_sequences, train_targets, TARGET_PROPERTY, MAX_LENGTH)\n",
    "val_dataset = ProteinDataset(val_sequences, val_targets, TARGET_PROPERTY, MAX_LENGTH)\n",
    "test_dataset = ProteinDataset(test_sequences, test_targets, TARGET_PROPERTY, MAX_LENGTH)\n",
    "    \n",
    "    # Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = ProteinLSTM(\n",
    "        vocab_size=21,  # 20 amino acids + 1 padding token\n",
    "        embed_dim=128,\n",
    "        hidden_size=256,\n",
    "        num_layers=3,\n",
    "        dropout_rate=0.5\n",
    "    )\n",
    "\n",
    "# Train model\n",
    "trained_model, train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE\n",
    ")\n",
    "\n",
    "predictions, true_values, mse, mae, r2 = evaluate_model(\n",
    "    trained_model, test_loader, test_dataset, TARGET_PROPERTY\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plot_results(train_losses, val_losses, predictions1, true_values, TARGET_PROPERTY)\n",
    "\n",
    "# Save model\n",
    "torch.save(trained_model.state_dict(), f'lstm_model_{TARGET_PROPERTY}_3.pth')\n",
    "print(f\"\\nModel saved as 'lstm_model_{TARGET_PROPERTY}.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32d9071-070e-49d8-bfcd-7d8a800f05e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>brightness</th>\n",
       "      <th>em_max</th>\n",
       "      <th>ex_max</th>\n",
       "      <th>ext_coeff</th>\n",
       "      <th>lifetime</th>\n",
       "      <th>maturation</th>\n",
       "      <th>pka</th>\n",
       "      <th>stokes_shift</th>\n",
       "      <th>qy</th>\n",
       "      <th>agg</th>\n",
       "      <th>switch_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>MVSKGEELFTGVVPILVEMDGDVNGRKFSVRGVGEGDATHGKLTLK...</td>\n",
       "      <td>-0.516789</td>\n",
       "      <td>-1.357357</td>\n",
       "      <td>-1.875798</td>\n",
       "      <td>-0.814071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.323540</td>\n",
       "      <td>0.923046</td>\n",
       "      <td>-0.056729</td>\n",
       "      <td>m</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>-0.802832</td>\n",
       "      <td>-0.408006</td>\n",
       "      <td>-0.214689</td>\n",
       "      <td>-1.192834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.878962</td>\n",
       "      <td>-0.403015</td>\n",
       "      <td>-0.465539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>MRSSKNVIKEFMRFKVRMEGTVNGHEFEIEGEGEGRPYEGHNTVKL...</td>\n",
       "      <td>-1.040228</td>\n",
       "      <td>0.883734</td>\n",
       "      <td>0.758032</td>\n",
       "      <td>-0.257845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074367</td>\n",
       "      <td>-1.725418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>MSKGEELFTGIVPVLIELDGDVHGHKFSVRGEGEGDADYGKLEIKF...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.516948</td>\n",
       "      <td>-0.184759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.641706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MALSKQEIKKEMTMDYVMDGCVNGHSFTVKGDGAGKPYEGHQRLSL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.610327</td>\n",
       "      <td>-0.244619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.694749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sequence  brightness    em_max  \\\n",
       "558  MVSKGEELFTGVVPILVEMDGDVNGRKFSVRGVGEGDATHGKLTLK...   -0.516789 -1.357357   \n",
       "149  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   -0.802832 -0.408006   \n",
       "184  MRSSKNVIKEFMRFKVRMEGTVNGHEFEIEGEGEGRPYEGHNTVKL...   -1.040228  0.883734   \n",
       "291  MSKGEELFTGIVPVLIELDGDVHGHKFSVRGEGEGDADYGKLEIKF...         NaN -0.516948   \n",
       "30   MALSKQEIKKEMTMDYVMDGCVNGHSFTVKGDGAGKPYEGHQRLSL...         NaN -0.610327   \n",
       "\n",
       "       ex_max  ext_coeff  lifetime  maturation       pka  stokes_shift  \\\n",
       "558 -1.875798  -0.814071       NaN         NaN  0.323540      0.923046   \n",
       "149 -0.214689  -1.192834       NaN         NaN  1.878962     -0.403015   \n",
       "184  0.758032  -0.257845       NaN         NaN       NaN      0.074367   \n",
       "291 -0.184759        NaN       NaN         NaN       NaN     -0.641706   \n",
       "30  -0.244619        NaN       NaN         NaN       NaN     -0.694749   \n",
       "\n",
       "           qy  agg switch_type  \n",
       "558 -0.056729    m           b  \n",
       "149 -0.465539  NaN           b  \n",
       "184 -1.725418  NaN           b  \n",
       "291       NaN    m           b  \n",
       "30        NaN    t           b  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fpgen.prop_prediction.dataset import FPbase\n",
    "dataset = FPbase('dataset.csv')\n",
    "dataset.to_train_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f01f60aa-9d66-4792-ab27-c5f020aa2bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shapes: [(32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (32,), (7,), (7,), (7,), (7,), (7,), (7,), (7,)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (167,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Convert back to original scale\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [np\u001b[38;5;241m.\u001b[39marray(p)\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m predictions])\n\u001b[0;32m---> 17\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m true_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(true_values)\n\u001b[1;32m     20\u001b[0m zv \u001b[38;5;241m=\u001b[39m get_regression_metrics(\n\u001b[1;32m     21\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mrescale_targets(predictions, TARGET_PROPERTY),\n\u001b[1;32m     22\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mrescale_targets(true_values, TARGET_PROPERTY)\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (167,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, targets in test_loader:\n",
    "        sequences, targets = sequences.to(device), targets.to(device)\n",
    "        batch_predictions = model(sequences).squeeze()\n",
    "\n",
    "        predictions.extend(batch_predictions.cpu().numpy())\n",
    "        true_values.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Convert back to original scale\n",
    "print(\"Prediction shapes:\", [np.array(p).shape for p in predictions])\n",
    "predictions = np.array(predictions)\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "zv = get_regression_metrics(\n",
    "    dataset.rescale_targets(predictions, TARGET_PROPERTY),\n",
    "    dataset.rescale_targets(true_values, TARGET_PROPERTY)\n",
    ")\n",
    "print(f'\\t RMSE: {zv[\"rmse\"]}')\n",
    "print(f'\\t MAE: {zv[\"mae\"]}')\n",
    "print(f'\\t R2: {zv[\"r2\"]}')\n",
    "print(f'\\t MAE (med.): {zv[\"mae_median\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "117e72d8-5f1b-4e7c-8ef6-2fa699b05a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset.csv...\n",
      "Loaded 980 rows\n",
      "Columns: ['sequence', 'brightness', 'em_max', 'ex_max', 'ext_coeff', 'lifetime', 'maturation', 'pka', 'stokes_shift', 'qy', 'agg', 'switch_type']\n",
      "ForwardTrackData(sequence=tensor([[[-39.2500, -39.2500, -39.5000,  ..., -39.2500, -39.5000, -39.5000],\n",
      "         [-38.5000, -38.5000, -38.5000,  ..., -38.5000, -38.5000, -38.5000],\n",
      "         [-41.2500, -41.2500, -41.2500,  ..., -41.2500, -41.2500, -41.2500],\n",
      "         ...,\n",
      "         [-38.7500, -38.7500, -38.7500,  ..., -38.7500, -38.7500, -38.7500],\n",
      "         [-36.5000, -36.5000, -36.5000,  ..., -36.5000, -36.5000, -36.5000],\n",
      "         [-35.5000, -35.5000, -35.5000,  ..., -35.5000, -35.5000, -35.5000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None) tensor([[[ 0.0097, -0.0044,  0.0015,  ...,  0.0047, -0.0040, -0.0087],\n",
      "         [-0.0054,  0.0091,  0.0352,  ...,  0.0341,  0.0125,  0.0333],\n",
      "         [-0.0214, -0.0059,  0.0173,  ...,  0.0481, -0.0115,  0.0243],\n",
      "         ...,\n",
      "         [ 0.0001,  0.0259, -0.0046,  ..., -0.0070,  0.0058, -0.0132],\n",
      "         [ 0.0105,  0.0493,  0.0030,  ...,  0.0065, -0.0036,  0.0142],\n",
      "         [ 0.0131,  0.0098, -0.0017,  ..., -0.0138, -0.0299, -0.0427]]],\n",
      "       device='cuda:0') torch.Size([1, 240, 960]) 238\n"
     ]
    }
   ],
   "source": [
    "CSV_FILE = 'dataset.csv'  # Update this path if needed\n",
    "TARGET_PROPERTY = 'em_max'  # Change this to predict different properties\n",
    "\n",
    "# Load data\n",
    "sequences, targets = load_data(CSV_FILE, TARGET_PROPERTY)\n",
    "protein = ESMProtein(sequences[1])\n",
    "client = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\") # or \"cpu\"\n",
    "protein_tensor = client.encode(protein)\n",
    "logits_output = client.logits(\n",
    "   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    ")\n",
    "print(logits_output.logits, logits_output.embeddings, logits_output.embeddings.shape, len(sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655fab1-8349-42a6-be04-939ceeac732c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
